# iris ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì¸ê³µì‹ ê²½ë§ ëª¨ë¸  

## ì•Œì•„ì•¼ í•˜ëŠ” ê°œë…  
### 1. iris  ë°ì´í„°ì…‹ ê¸°ë³¸  
**MLPClassifier**ëŠ” ë¶„ë¥˜ì—ì„œ `í´ë˜ìŠ¤ ê°œìˆ˜ = ì¶œë ¥ ë‰´ëŸ° ìˆ˜`ë¡œ ë§ì¶˜ë‹¤. (ì¶œë ¥ì¸µì€ ì†Œí”„íŠ¸ë§¥ìŠ¤)  
- iris.data         : (150, 4) ì‹¤ìˆ˜ íŠ¹ì§•  
- iris.target       : (150,) ì •ìˆ˜ ë ˆì´ë¸” ë²¡í„° {0, 1, 2}  
  - `(150,)` í‘œê¸°ëŠ” **1ì°¨ì› ë°°ì—´**ì— ì›ì†Œê°€ 150ê°œ ë¼ëŠ” ëœ»  
  - `(150,1)` ì€ **2ì°¨ì›**(150í–‰ 1ì—´) ì—´ ë²¡í„°
    - 3 classes : (150,)ì€ ë„˜íŒŒì´ ë°°ì—´ì˜ ëª¨ì–‘ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œê¸°ì´ë©°, ë’¤ì˜ ì‰¼í‘œëŠ” ì°¨ì›ì´ 1ê°œì„ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ë¯€ë¡œ 1ì°¨ì› ë°°ì—´ì— ì›ì†Œê°€ 150ê°œë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ë§Œì•½ (150, 1) ì´ ëœë‹¤ë©´ 2ì°¨ì›(2D), 150í–‰ 1ì—´ì˜ ì„¸ë¡œ(ì—´) ë²¡í„°ê°€ ëœë‹¤.  
- iris.target_names : ë ˆì´ë¸” ì´ë¦„ ë°°ì—´ â†’ ['setosa', 'versicolor', 'virginica']  
- **ë¶„ë¥˜ ì¶œë ¥ í¬ê¸°**: í´ë˜ìŠ¤ ê°œìˆ˜(=3) â†’ ì¶œë ¥ ìœ ë‹› 3ê°œ(softmax í™•ë¥ )  

### 2. ë°ì´í„° ë¶„í•  ê´€ë ¨  
- **train** : ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë°ì´í„°  
- **validation** : í•™ìŠµ ì¤‘ ì„±ëŠ¥ì„ í™•ì¸/íŠœë‹(ì¡°ê¸° ì¢…ë£Œ ë“±)ì— ì“°ëŠ” ë°ì´í„°  
- **test** : ìµœì¢… ì„±ëŠ¥ í™•ì¸ìš©(í•™ìŠµ/íŠœë‹ì— ì‚¬ìš© ê¸ˆì§€)  
- `train_test_split(..., test_size=0.2)` â†’ **8:2 ë¶„í• ** (ì˜ˆ: 120/30)  
- `random_state` (=seed ê³ ì •)  
  - ë°ì´í„°ë¥¼ ì„ëŠ” ìˆœì„œ(ë¶„í•  ê²°ê³¼)ë‚˜ ì´ˆê¸° ê°€ì¤‘ì¹˜ ë“±ì˜ **ë‚œìˆ˜ì„±ì„ ê³ ì •**í•´ **ì¬í˜„ì„±** í™•ë³´  
  - ì˜ˆ: `train_test_split(..., ramdom_state=42)`, `MLPClassifier(..., random_state=42)`  

### 3. í•™ìŠµ ë£¨í”„ì˜ êµ¬ì„± ìš”ì†Œ  
- **epoch** : **í›ˆë ¨ ë°ì´í„° ì „ì²´ë¥¼ 1ë°”í€´** ë‹¤ í•™ìŠµí•œ ê²ƒ  
  - Keras `fit(epoch=E)` â†’ ì •í™•íˆ Eë²ˆ  
  - scikit-learnì˜ `MLPClassifier(max_iter=...)`:  
    - `solver='adam'` / `sgd`ì¼ ë•Œ **ë°ì´í„°ì— ëŒ€í•œ ë°˜ë³µ íšŸìˆ˜(ì—í­ ìˆ˜ì™€ ìœ ì‚¬)** ì˜ë¯¸  
- **batch(ë¯¸ë‹ˆë°°ì¹˜)** : í•œ ë²ˆì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ì— ì‚¬ìš©í•˜ëŠ” **ë°ì´í„° ë¬¶ìŒ** (ì‘ì„ìˆ˜ë¡ ìì£¼ ê³ ì¹˜ê³ , í´ìˆ˜ë¡ ì•ˆì •ì ì´ë‹¤.)  
- **batch_size** :  
  - *Keras* : `fit(..., batch_size=32)` ì²˜ëŸ¼ ì§ì ‘ ì§€ì •  
  - scikit-learn MLP : ê¸°ë³¸ `batch_size='auto'` (ë³´í†µ `min(200, n_samples)`)  
- **shuffle** : ë§¤ epochë§ˆë‹¤ ë°ì´í„°ë¥¼ ì„ì–´ í•™ìŠµ ì•ˆì •í™”(ì¼ë°˜í™”) ë„ì›€  

### 4. ì†ì‹¤/ì—­ì „íŒŒ/ì§€í‘œ  
- **loss(ì†ì‹¤)** : ëª¨ë¸ì˜ í‹€ë¦° ì •ë„  
  - ë‹¤ì¤‘ë¶„ë¥˜ : **í¬ë¡œìŠ¤í”¼ì—”íŠ¸ë¡œí”¼**(Keras: `categorical_crossentropy` / `sparse_categorical_crossentropy`, scikit-learn MLP: **log_loss** ë‚´ë¶€ ì‚¬ìš©)  
  - `sparse_categorical_crossentropy`: **ì •ìˆ˜ ë ˆì´ë¸”**ì— ì‚¬ìš© (ì›-í•« í•„ìš” ì—†ìŒ)  
  - `categorical_crossentropy`: **ì›-í•« ë ˆì´ë¸”**ì— ì‚¬ìš©  
- **backprop(ì—­ì „íŒŒ)** : ì†ì‹¤ì„ ì¤„ì´ë„ë¡ ê° ê°€ì¤‘ì¹˜ì˜ **ê¸°ìš¸ê¸°(gradient)** ê³„ì‚°  
- **optimizer(ìµœì í™”)** : ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê·œì¹™ (ì˜ˆ: *Adam*, SGD)  
  - **learning rate**ê°€ ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´ì´ ëŠë¦¼  
- **accuracy(ì •í™•ë„)** : `ë§ì¶˜ ê°œìˆ˜ / ì „ì²´ ê°œìˆ˜`  
  - Keras `metrics=['accuracy']`ë©´ `evaluate`ê°€ `[loss, accuracy]` ë°˜í™˜  
  - scikit-learnì€ `model.score(X, y)`ê°€ ì •í™•ë„  

### 5. ê²€ì¦/ì¡°ê¸°ì¢…ë£Œ (early stopping)  
- **validation_split** :  
  - *Keras* `fit(validation_split=0.2)` â†’ í›ˆë ¨ ë°ì´í„° ì¤‘ **20%ë¥¼ ê²€ì¦**ìœ¼ë¡œ ìë™ ë¶„ë¦¬  
  - scikit-learn MLPëŠ” `early_stooping=True` ì¼ ë•Œ ë‚´ë¶€ì ìœ¼ë¡œ `validation_fraction` (ê¸°ë³¸ 0.1)ì„ ì‚¬ìš©  
- **patience** :  
  - ê°œì„ ì´ ì—†ë”ë¼ë„ ëª‡ epoch ë” ê¸°ë‹¤ë¦´ì§€ **  
  - ì˜ˆ: `EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)`  
- **val_accuracy / val_loss** : ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ ì •í™•ë„/ì†ì‹¤(ê³¼ì í•© ì§„ë‹¨ì— ìœ ìš©)  

### 6. ëª¨ë¸ êµ¬ì¡°/í™œì„±í™”/ì •ê·œí™”  
- **units(ìœ ë‹›)** = ë‰´ëŸ° 1ê°œ, `Dense(units=N)` â†’ ì¶œë ¥ ì°¨ì› `N`  
- **activation(í™œì„±í™”)** :  
  - **ReLU**(ê¸°ë³¸, ë¹ ë¥´ê³  íš¨ê³¼ì ), **sigmoid**(ì¶œë ¥ì´ 0~1), **softmax**(ë‹¤ì¤‘ë¶„ë¥˜ í™•ë¥ )  
- **regularization(ì •ê·œí™”)** :  
  - scikit-learn MLPì˜ `alpha`ëŠ” **L2 ê·œì œ ì„¸ê¸°**(ê³¼ì í•© ì™„í™”)  
  - Kerasì—ì„œë„ `kernel_regularizer=l2(...)`, `dropout` ë“± ì‚¬ìš© ê°€ëŠ¥  
- **ìŠ¤ì¼€ì¼ë§(ì¤‘ìš”)** :  
  - MLPëŠ” ì…ë ¥ ìŠ¤ì¼€ì¼ì— ë¯¼ê° â†’ **í‘œì¤€í™”** ê¶Œì¥ (`StandardScaler` íŒŒì´í”„ë¼ì¸)  

### 7. ì˜ˆì¸¡ê³¼ í•´ì„  
- **predict** : ê° ìƒ˜í”Œì˜ **í´ë˜ìŠ¤ ë¼ë²¨** (Irisì—ì„  0/1/2)  
- **predict_proba** : ê° í´ë˜ìŠ¤ì˜ **í™•ë¥  ë²¡í„°**(softmax ì¶œë ¥, í•©=1)  
- **confusion matrix(í˜¼ë™í–‰ë ¬)** : **ì‹¤ì œ vs ì˜ˆì¸¡** ë¹ˆë„í‘œ(ëŒ€ê°ì„ =ì •ë‹µ, ë¹„ëŒ€ê°=ì˜¤ë¶„ë¥˜)  
- (í•„ìš”ì‹œ) **precision/recall/F1**ë¡œ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ í™•ì¸  

> *Tips*:  
> - **ì¬í˜„ì„±**ì´ ì¤‘ìš”í•˜ë©´ ë°ì´í„° ë¶„í• ê³¼ ëª¨ë¸ ë‘˜ ë‹¤ `random_state`ë¥¼ ê³ ì •í•˜ì„¸ìš”.  
> - scikit-learn MLPë¥¼ ì“¸ ë• **ì…ë ¥ í‘œì¤€í™”**(ì˜ˆ: `make_pipeline(StandardScaler(), MLPClassifier(...))`)ê°€ ìˆ˜ë ´/ì„±ëŠ¥ì— í¬ê²Œ ë„ì›€ë©ë‹ˆë‹¤.  
<br>
<br>


## ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyper parameters)/ê¸°ë³¸ê°’  
```
hidden_layer_sizes=(10,10): ì€ë‹‰ì¸µ êµ¬ì¡°  
activation='relu': í™œì„±í™” í•¨ìˆ˜  
solver='adam': ìµœì í™” ë°©ì‹(ì‘ì€/ì¤‘ê°„ ë°ì´í„°ì— ì í•©)  
alpha=1e-4: L2 ê·œì œ(ê³¼ì í•© ì™„í™”)  
max_iter=1000: ìµœëŒ€ ë°˜ë³µ ìˆ˜  
random_state=None: ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ê°€ ë§¤ ì‹¤í–‰ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ â†’ ì¬í˜„ì„± ì›í•˜ë©´ random_state=42 ì§€ì •  
```
<br>
<br>


## ì½”ë“œ ìƒì„¸ í•´ì„¤  
### 1 cell  
```
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
# print(iris)

model = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000)
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)
model.fit(X_train, y_train)
print("model accuracy:", model.score(X_test, y_test))
print("Predictions:", model.predict(X_test))
```
**ë¬´ì—‡ì„ í•˜ëŠ” ì½”ë“œì¸ê°€?**
1. Scikit-learnì˜ MLPClassifier(ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ )ë¡œ Iris(ë¶“ê½ƒ) ë°ì´í„°ì…‹ì„ í•™ìŠµí•œë‹¤.  
2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ â†’ í•™ìŠµ â†’ ì •í™•ë„(accuracy) í‰ê°€ â†’ í´ë˜ìŠ¤ ì˜ˆì¸¡ê°’ ì¶œë ¥í•œë‹¤.  
<br>

`from sklearn.~`  
ë¶„ë¥˜ìš© ë‹¤ì¸µí¼ì…‰íŠ¸ë¡ (MLPClassifier), iris ë°ì´í„° ë¡œë”(load_iris), ë°ì´í„°  ë¶„í•  ìœ í‹¸(train_test_split)ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.  

`iris = load_iris()`  
ë°ì´í„°ì…‹ì„ ë¡œë“œí•œë‹¤.  

`model = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000)`  
MLP ë¶„ë¥˜ê¸°ë¥¼ ìƒì„±í•œë‹¤.  
- `iris.data` ì…ë ¥ íŠ¹ì„±ì´ 4ê°œì—¬ì„œ, input ë‰´ëŸ°ì€ 4ê°œë¡œ ì‹œì‘í•œë‹¤.  
- `hidden_layer_sizes=(10, 10)` ì€ ì€ë‹‰ì¸µ 2ê°œì´ë©°, ê° 10ê°œì˜ ë‰´ëŸ°ì„ ìƒì„±í•œë‹¤.  
- `iris.target` ì¶œë ¥ íŠ¹ì„±ì´ 3ê°œì—¬ì„œ, output ë‰´ëŸ°ì€ 3ê°œê°€ ëœë‹¤.  
![neural_network](./images/neural_network.png)  
- ìµœëŒ€ ë°˜ë³µ 1000 epoch(ì •í™•íˆëŠ” scikit-learnì˜ ë°˜ë³µ ìŠ¤í…)  
- ê¸°ë³¸ê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤.  

| key                | value    |
| ------------------ | -------- |
| activation         | relu     |
| solver             | adam     |
| alpha              | 1e-4(L2) |
| learning_rate_init | 1e-3     |
| early_stopping     | False    |
| random_state       | None     | 

`X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)`  
**iris.data**ì™€ **iris.target**ì—ëŠ” 150ê°œì˜ ìƒ˜í”Œì´ ìˆê³ , test_size=0.2 ëŠ” í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ì´ **20%**ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  
20% ì¦‰ 8:2 ë¹„ìœ¨ë¡œ ë¶„í• í•˜ë©´ 150ê°œì˜ ìƒ˜í”Œì˜ í…ŒìŠ¤íŠ¸ëŠ” 30ê°œê°€ ëœë‹¤. (í•™ìŠµ 120, í…ŒìŠ¤íŠ¸ 30)  
(ps. _test_size_ë¥¼ ì •ìˆ˜ë¡œ ì£¼ë©´ ì •í™•íˆ ê·¸ ê°œìˆ˜ë§Œí¼ í…ŒìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•œë‹¤. ex: _test_size=30_)  
`random_state=42` â†’ í•­ìƒ ê°™ì€ ë¶„í• ì„ ì‚¬ìš©í•˜ë„ë¡ ëœë¤í•œ ë™ì‘ì˜ seedë¥¼ ê³ ì •í•œë‹¤. `train_test_split`ì€ ë°ì´í„°ë¥¼ ì„ì–´ì„œ(ì…”í”Œ) í•™ìŠµ/í…ŒìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ”ë°, ì´ë•Œ ì“°ëŠ” ë‚œìˆ˜ ìƒì„±ê¸°ì˜ ì‹œì‘ê°’(seed)ì„ 42ë¡œ ê³ ì •í•˜ë©´ í•­ìƒ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì„ì´ê¸° ë•Œë¬¸ì— ë§¤ë²ˆ ê°™ì€ ë¶„í•  ê²°ê³¼(ê°™ì€ ìƒ˜í”Œì´ í•™ìŠµ/í…ŒìŠ¤íŠ¸ë¡œ ê°)ë¥¼ ì–»ëŠ”ë‹¤. : ê°™ì€ ì½”ë“œ/ë°ì´í„°ë¡œ í•­ìƒ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•œ ì¬í˜„ì„± í•„ìš”ì‹œ ì‚¬ìš©í•œë‹¤. ex, ì‹¤í—˜ ê¸°ë¡/ë¹„êµ  

`model.fit(X_train, y_train)`  
ì‹ ê²½ë§ í•™ìŠµì„ ì‹œì‘í•œë‹¤. ì˜ˆì¸¡ > í‹€ë¦° ê²ƒ í™•ì¸(loss) > ìˆ˜ì •(ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸) ìˆœì„œë¥¼ ê³„ì†í•´ì„œ ë°˜ë³µí•œë‹¤.  
1. ë°ì´í„°ë¥¼ ì„ê³ (shuffle) Adam ì˜µí‹°ë§ˆì´ì €ë¡œ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•œë‹¤.  
2. ì˜ˆì¸¡ `ìˆœì „íŒŒ, forward`  
- ì…ë ¥(4) â†’ ì€ë‹‰ì¸µ(10) â†’ ì€ë‹‰ì¸µ(10) â†’ ì¶œë ¥(3) ìˆœì„œë¡œ ê°’ì„ ë³´ë‚´ ê° í´ë˜ìŠ¤ í™•ë¥ ì„ ê³„ì‚°í•œë‹¤.  
3. ì˜¤ì°¨ í™•ì¸ `loss`  
- ì˜ˆì¸¡ê³¼ ì •ë‹µ(y_train)ì„ ë¹„êµí•´ì„œ ì˜¤ì°¨(loss)ë¥¼ êµ¬í•œë‹¤.  
4. ì˜¤ì°¨ ì¶”ì  `ì—­ì „íŒŒ, backprop`  
- ì˜¤ì°¨ê°€ ê° ê°€ì¤‘ì¹˜, ë°”ì´ì–´ìŠ¤ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë°›ì•˜ëŠ”ì§€ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.  
5. ì—…ë°ì´íŠ¸  
- Adam ì˜µí‹°ë§ˆì´ì €ê°€ ê¸°ìš¸ê¸°ë§Œí¼ ê°€ì¤‘ì¹˜/ë°”ì´ì–´ìŠ¤ë¥¼ ì‚´ì§ ë³€ê²½í•œë‹¤.  
- ì—¬ëŸ¬ ë°°ì¹˜, ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•œë‹¤.  
ìœ„ ê³¼ì •ì„ ê³„ì† ë°˜ë³µí• ìˆ˜ë¡ ì˜¤ì°¨ê°€ ì ì  ì ì–´ì ¸ ì •ë‹µ í´ë˜ìŠ¤ì˜ ì ìˆ˜(í™•ë¥ )ê°€ ì˜¬ë¼ê°€ë„ë¡ ì¡°ì •ëœë‹¤.  

`print("model accuracy:", model.score(X_test, y_test))`  
í…ŒìŠ¤íŠ¸ì…‹ **ì •í™•ë„**ë¥¼ ì¶œë ¥í•œë‹¤. (scoreëŠ” ê¸°ë³¸ìœ¼ë¡œ accuracyë¥¼ ë°˜í™˜)  
ì¶œë ¥ì€ 0ê³¼ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ì´ë©°, `ë§ì¶˜ ê°œìˆ˜ / ì „ì²´ ê°œìˆ˜`ë¡œ ê³„ì‚°í•œë‹¤.  

`print("Predictions:", model.predict(X_test))`  
í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•œ **ì˜ˆì¸¡ ë ˆì´ë¸” ë°°ì—´**ì„ ì¶œë ¥í•œë‹¤.  
ì¶œë ¥ì€ `ê¸¸ì´ = í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜`ì´ê³ , ê° ë ˆì´ë¸”ì˜ ìˆ«ì(0, 1, 2)ëŠ” í’ˆì¢… í´ë˜ìŠ¤ë¥¼ ì˜ë¯¸í•œë‹¤.  
> í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³´ê³  ì‹¶ë‹¤ë©´ `names = iris.target_names` ì„ ì‚¬ìš©í•œë‹¤.  
> > í´ë˜ìŠ¤ ì´ë¦„ : ['setosa', 'versicolor', 'virginica']   
<br>

ğŸ’¡ **Point!**
- score() = ì •í™•ë„ = ë§ì¶˜ë¹„ìœ¨  
- predict() = ê° ìƒ˜í”Œì˜ ì˜ˆì¸¡ í´ë˜ìŠ¤ ë°°ì—´  
<br>


### 2 cell
```
from sklearn.metrics import confusion_matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, model.predict(X_test)))
```
**ë¬´ì—‡ì„ í•˜ëŠ” ì½”ë“œì¸ê°€?**  
1. confusion matrixë¥¼ ê³„ì‚°í•´ì„œ ì¶œë ¥í•œë‹¤.  
2. `X_test`ì— ëŒ€í•œ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ì¦‰ì„ì—ì„œ ë§Œë“¤ê³ (`model.predict(X_test)`), ê·¸ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì •ë‹µ `y_test`ë¥¼ ë¹„êµí•´ confusion matrixë¥¼ ê³„ì‚°í•œ ë’¤ ì¶œë ¥í•œë‹¤.  
3. confusion matrixëŠ” í–‰=ì‹¤ì œ í´ë˜ìŠ¤, ì—´=ì˜ˆì¸¡ í´ë˜ìŠ¤ ì´ë©°, ê° ì¹¸ì€ í•´ë‹¹ ì¡°í•©ì˜ ê°œìˆ˜ì´ë‹¤.  
<br>

`from sklearn.matrix import confusion_matrix`  
scikit-learnì˜ metrics ëª¨ë“ˆì—ì„œ confusion_matrixë¥¼ ê°€ì ¸ì™€ì„œ, confusion_matrix(...)ë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ ëœë‹¤.  
`print("Confusion Matrix:\n", confusion_matrix(y_test, model.predict(X_test)))`  
- ì‹¤í–‰ ìˆœì„œ  
  - `model.predict(X_test)` â†’ í…ŒìŠ¤íŠ¸ì…‹ ê° ìƒ˜í”Œì˜ ì˜ˆì¸¡ ë ˆì´ë¸” ë°°ì—´ì„ ë§Œë“ ë‹¤.  
  - `confusion_matrix(y_test, <ì˜ˆì¸¡ë°°ì—´>)` â†’ í–‰ = ì‹¤ì œ ë ˆì´ë¸”(y_test), ì—´ = ì˜ˆì¸¡ ë ˆì´ë¸” ê¸°ì¤€ì˜ **ë¹ˆë„í‘œ(ì •ìˆ˜ í–‰ë ¬)**ë¥¼ ê³„ì‚°í•œë‹¤.  
    - í¬ê¸° : `n_classes x n_classes` (irisëŠ” ë³´í†µ 3x3)  
    - ëŒ€ê°ì„  = ë§ì¶˜ ê°œìˆ˜, ëŒ€ê°ì„  ë°– = ì˜¤ë¶„ë¥˜ ê°œìˆ˜  
    - ë ˆì´ë¸” ìˆœì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì •ë ¬ëœ ê³ ìœ ê°’ ìˆœì„œë¥¼ ì‚¬ìš©í•œë‹¤.  
<br>


### 3 cell
```
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Input

model = Sequential()
model.add(Input(shape=(4,))) # input layer
model.add(Dense(50, activation='sigmoid')) # hidden layer
model.add(Dense(30, activation='sigmoid')) # hidden layer
model.add(Dense(3, activation='softmax')) # output layer
model.summary()
```
<br>

**ë¬´ì—‡ì„ í•˜ëŠ” ì½”ë“œì¸ê°€?**  
1. Kerasë¡œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP) ë¶„ë¥˜ ëª¨ë¸ì˜ êµ¬ì¡°ë§Œ ì •ì˜í•˜ê³  ìš”ì•½ì„ ì¶œë ¥í•œë‹¤.  
<br>

`from tensorflow.keras import Sequential`  
Kerasì˜ Sequential í´ë˜ìŠ¤ëŠ” layerë“¤ì„ **ìœ„ì—ì„œ ì•„ë˜ë¡œ ìˆœì°¨ì ìœ¼ë¡œ** ìŒ“ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•œë‹¤.  

`from tensorflow.keras.layers import Dense, Input`  
Dense : ì™„ì „ ì—°ê²°(fully connected) ì¸µ  
Input : ì…ë ¥ í…ì„œì˜ ëª¨ì–‘(shape)ì„ ì„ ì–¸í•˜ëŠ” ì¸µ(ë°°ì¹˜ ì°¨ì› ì œì™¸)  

`model = Sequential()`  
ë¹ˆ Sequential ëª¨ë¸ì„ ìƒì„±í•œë‹¤. ì´ì œ add()ë¡œ ì¸µì„ ì°¨ë¡€ëŒ€ë¡œ ë¶™ì¼ ìˆ˜ ìˆë‹¤.  

`model.add(Input(shape=(4,)))`  
ì…ë ¥ì˜ í˜•ìƒì„ ì§€ì •í•œë‹¤. ê° ìƒ˜í”Œì€ ê¸¸ì´ 4ì˜ ë²¡í„°ë¥¼ ê°–ëŠ”ë‹¤.  
ë°°ì¹˜ ì°¨ì›ì€ ì œì™¸í•˜ë¯€ë¡œ, ì‹¤ì œ ì…ë ¥ ëª¨ì–‘ì€ `(batch_size, 4)`ê°€ ëœë‹¤.  
ì—¬ê¸°ì„œëŠ” í•™ìŠµ íŒŒë¼ë¯¸í„°ì¸ ê°€ì¤‘ì¹˜ë¥¼ ê°–ì§€ ì•ŠëŠ”ë‹¤.  

`model.add(Dense(50, activation='sigmoid'))`  
ìœ ë‹› 50ê°œì˜ ì€ë‹‰ì¸µì„ ì¶”ê°€í•œë‹¤.  
í™œì„±í™” í•¨ìˆ˜ëŠ” `sigmoid`ì´ë©°, ì´ì „ ì¸µì˜ ëª¨ë“  ë‰´ëŸ°ê³¼ ì™„ì „ ì—°ê²°ëœë‹¤.  

`model.add(Dense(30, activation='sigmoid'))`  
ìœ ë‹› 30ê°œì˜ ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µìœ¼ë¡œ, í™œì„±í™” í•¨ìˆ˜ëŠ” `sigmoid`ì´ë‹¤.  

`model.add(Dense(3, activation='softmax'))`  
ìœ ë‹› 3ê°œì˜ ì¶œë ¥ì¸µ, `softmax`ë¡œ ê° í´ë˜ìŠ¤ì˜ í™•ë¥ (í•©=1)ì„ ì¶œë ¥í•œë‹¤.  
ë³´í†µ 3-classses ë¶„ë¥˜ ë¬¸ì œë¥¼ ëŒ€ì‘í•  ë•Œ ì´ëŸ¬í•œ í˜•íƒœë¥¼ ì‚¬ìš©í•œë‹¤.  

`model.summary()`  
ëª¨ë¸ êµ¬ì¡° ìš”ì•½ì„ ì¶œë ¥í•œë‹¤. - ê° ì¸µì˜ ì¶œë ¥ í˜•íƒœ, íŒŒë¼ë¯¸í„° ìˆ˜, ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ë“±.  

- ì°¸ê³ ) íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° ì˜ˆì‹œ
  - Dense(50) : (ì…ë ¥ 4 x ìœ ë‹› 50) + ë°”ì´ì–´ìŠ¤ 50 = 4*50 + 50 = 250  
  - Dense(30) : (50 x 30) + 30 = 1530  
  - Dense(3)  : (30 x 3) + 3 = 93  
  - ì´í•© : 250 + 1530 + 93 = 1873 (Inputì¸µì€ íŒŒë¼ë¯¸í„°ê°€ ì—†ë‹¤.)  
    - ì—¬ê¸°ì„œ ìœ ë‹›(unit)ì´ë€ ë‰´ëŸ°(node) 1ê°œë¥¼ ì˜ë¯¸í•œë‹¤.  


### 4 cell
```
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=100, batch_size=50)
```
<br>

**ë¬´ì—‡ì„ í•˜ëŠ” ì½”ë“œì¸ê°€?**  
1. `í•™ìŠµ ê·œì¹™(ìµœì í™”/ì†ì‹¤/ì§€í‘œ)`ì„ ì„¤ì •í•˜ì—¬ ì–´ë–¤ ì§€í‘œ(accuracy)ë¡œ ëª¨ë¸ì„ í›ˆë ¨Â·í‰ê°€í• ì§€ ì§€ì •í•œë‹¤.  
2. í›ˆë ¨ ë°ì´í„°ë¥¼ ë°°ì¹˜ í¬ê¸° 50ìœ¼ë¡œ ë‚˜ëˆ  100 ì—í­ ë™ì•ˆ ë°˜ë³µ í•™ìŠµí•˜ë©°, ì†ì‹¤ì„ ì¤„ì´ë„ë¡ ê°€ì¤‘ì¹˜/ë°”ì´ì–´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì‹¤ì œ í•™ìŠµì„ ì‹¤í–‰í•œë‹¤.  
<br>

`model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])`  
ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í•™ìŠµÂ·í‰ê°€í• ì§€ ã„¹ì§€ì •í•˜ì—¬ í›ˆë ¨ ì„¤ì •ì„ ê³ ì •í•œë‹¤.  
- `optimizer='adam'`  
  - Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—­ì „íŒŒë¡œ ê³„ì‚°í•œ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•´ ê°€ì¤‘ì¹˜/ë°”ì´ì–´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.  
- `loss='sparse_categorical_crossentropy'`  
  - ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ìš© í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ë¥¼ ì†ì‹¤í•œë‹¤.  
  - ì •ìˆ˜ ë ˆì´ë¸”ì„ ë°”ë¡œ ë„£ì„ ë•Œ ì‚¬ìš©í•œë‹¤.  
  - ë ˆì´ë¸”ì´ ì›-í•« ë²¡í„°ë¼ë©´ categorical_crossentropyë¥¼ ì¨ì•¼ í•œë‹¤.  
- `metrics=['accuracy']`  
  - í›ˆë ¨(ë° ê²€ì¦/í‰ê°€) ì‹œ ì •í™•ë„ë¥¼ í•¨ê»˜ ê³„ì‚°í•´ ë¡œê·¸ì— ë³´ì—¬ì¤€ë‹¤.

`model.fit(X_train, y_train, epochs=100, batch_size=50)`  
ì‹¤ì œ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.  
- `epochs=100`
  - í›ˆë ¨ ë°ì´í„°ë¥¼ 100ë²ˆ ë°˜ë³µí•œë‹¤.  
- `batch_size=50`
  - í›ˆë ¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì— 50ê°œì”© ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë‚˜ëˆ  ì—…ë°ì´íŠ¸í•œë‹¤.  
  - (iris ì˜ˆì‹œì—ì„œ X_trainì´ 120ê°œë¼ë©´) ì—í­ë‹¹ ìŠ¤í… ìˆ˜ = ceil(120/50) = 3  
    - 100 ì—í­ì´ë©´ ì´ ì•½ 300ë²ˆ(3x100) íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚œë‹¤.  
    - ë§ˆì§€ë§‰ ë°°ì¹˜ëŠ” 20ê°œë¡œ ìë™ ì²˜ë¦¬ëœë‹¤.  


### 5 cell
```
print(model.evaluate(X_test, y_test))
```
<br>

í•™ìŠµëœ Keras ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€í•œë‹¤.  
ì»´íŒŒì¼ì—ì„œ ì§€ì •í•œ lossì™€ metricsë¥¼ ê³„ì‚°í•˜ê³ , ì„¤ì •ëœ `metrics=['accuracy']` ê¸°ì¤€ìœ¼ë¡œ `test_loss, test_accuracy`ë¥¼ ë°˜í™˜í•œë‹¤.  
ê·¸ í›„ `print(..)`ë¡œ ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•œë‹¤.  
<br>


### 6 cell
```
model.save('iris_model.keras')
```
<br>

í˜„ì¬ keras ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥í•œë‹¤. Kears 3ì˜ ê¶Œì¥ í¬ë§·ì¸ `.keras`(zipê¸°ë°˜ ë‹¨ì¼ íŒŒì¼)ë¡œ ì €ì¥í•œë‹¤.  
ì €ì¥ë˜ëŠ” ì‚¬í•­ì€ ì•„ë˜ì™€ ê°™ë‹¤.  
- ëª¨ë¸ êµ¬ì¡°(ë ˆì´ì–´/í•˜ì´í¼íŒŒë¼ë¯¸í„°)  
- ê°€ì¤‘ì¹˜/ë°”ì´ì–´ìŠ¤ ê°’  
- í›ˆë ¨ ì„¤ì •(loss, optimizer, metrics)  
- ì˜µí‹°ë§ˆì´ì € ìƒíƒœ(í•™ìŠµì„ ì´ì–´ì„œ ì¬ê°œ ê°€ëŠ¥)  
í˜„ì¬ ì§ì—… ë””ë ‰í„°ë¦¬ì— `iris_model.keras`ë¼ëŠ” íŒŒì¼ë¡œ ì €ì¥ëœë‹¤. (ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°)  
**ì£¼ì˜: `.h5`(HDF5)ì™€ ì„œë¡œ í˜¸í™˜ë˜ì§€ ì•Šìœ¼ë‹ˆ ë‹¨ìˆœíˆ í™•ì¥ìë§Œ ë°”ê¿” ì“°ë©´ ì•ˆ ëœë‹¤.**
<br>


### 7 cell
```
from tensorflow.keras.models import load_model
loaded_model = load_model('iris_model.keras')
```
<br>

Kerasì—ì„œ ì €ì¥ëœ ëª¨ë¸ì„ íŒŒì¼ë¡œë¶€í„° ë³µì›í•˜ëŠ” í•¨ìˆ˜ load_modelì„ ê°€ì ¸ì˜¨ë‹¤.  
í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬ì— ìˆëŠ” `iris_model.keras`íŒŒì¼ì„ ì½ì–´ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ë¡œ ë³µì›í•´ loaded_model ë³€ìˆ˜ì— ë‹´ëŠ”ë‹¤.  
ëª¨ë¸ êµ¬ì¡°, ê°€ì¤‘ì¹˜/ë°”ì´ì–´ìŠ¤, compile ì„¤ì •(loss/metrics/optimizer), ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë“±ì´ ë³µì›ëœë‹¤.  
<br>


### 8 cell
```
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

print(loaded_model.evaluate(X_test, y_test))
```
<br>

**ë¬´ì—‡ì„ í•˜ëŠ” ì½”ë“œì¸ê°€?**  
1. ì €ì¥í•´ ë‘” Kerasëª¨ë¸(loaded_model)ì„ iris í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€í•´ì„œ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ì½”ë“œë‹¤.  
2. ìˆœì„œ  
- iris ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° -> í•™ìŠµ/í…ŒìŠ¤íŠ¸ 8:2 ë¶„í• (ì¬í˜„ì„± ìœ„í•´ random_state=24)  
- `loaded_model.evaluate(X_test, y_test)`ë¡œ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê³„ì‚°  
- `print(..)`ë¡œ `test_loss, test_accuracy` ì¶œë ¥  
<br>

`from sklearn.datasets import load_iris`  
scikit-learn ë‚´ì¥ iris ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ `load_iris`ë¥¼ importí•œë‹¤.  

`from sklearn.model_selection import train_test_split`  
ë°ì´í„°ì…‹ì„ í•™ìŠµìš©/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ `train_test_split`ì„ importí•œë‹¤.  

`iris = load_iris()`  
iris ë°ì´í„°ì…‹ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•´ì„œ iris ë³€ìˆ˜ì— ì €ì¥í•œë‹¤.  
ì£¼ìš” ì†ì„±ìœ¼ë¡œëŠ” `iris.data`(í˜•ìƒ `(150, 4)`ì¸ íŠ¹ì§•), `iris.target`(í˜•ìƒ `(150,)`ì¸ ì •ìˆ˜ ë ˆì´ë¸” 0/1/2)ì´ ìˆë‹¤.  

`X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)`  
íŠ¹ì§•ê³¼ ë ˆì´ë¸”ì„ í›ˆë ¨/í…ŒìŠ¤íŠ¸ë¡œ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆˆë‹¤.  
ì „ì²´ 150ê°œ ì¤‘ í›ˆë ¨ 120, í…ŒìŠ¤íŠ¸ 30ê°œê°€ ëœë‹¤.  
`random_state=42`ë¡œ ì…”í”Œ ì‹œë“œë¥¼ ê³ ì •í•´ í•­ìƒ ê°™ì€ ë¶„í• ì„ ì–»ëŠ”ë‹¤.  

`print(loaded_model.evaluate(X_test, y_test))`  
ì €ì¥ë˜ì–´ ìˆë˜ Keras ëª¨ë¸ `loaded_model`ì„ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€í•œë‹¤.  
`complie`ë•Œ ì§€ì •ëœ **ì†ì‹¤(loss)**ê³¼ **ì§€í‘œ(metrics)**ë¥¼ ê³„ì‚°í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.  
- ì´ ë•Œ ëª¨ë¸ì€ `metrics=['accuracy']`ì˜€ìœ¼ë¯€ë¡œ ë°˜í™˜ì€ `[test_loss, test_accuracy]`.  
`print(..)`ë¡œ ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•œë‹¤.  